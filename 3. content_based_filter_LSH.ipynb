{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSH for cosine similarity using random hyperplanes  \n",
    "1. Shigling to get the feature matrix.  \n",
    "2. Generate random hyperplanes, if two vectors **a** and **b** are on the same side of a hyperplane **H**, their dot product with the normal vector of the hyperplane **v** will have the same sign, use sketches with reduced dimension vectors to represent documents, which can reflect similarity. \n",
    "3. Locality-sensitive hashing, similar documents will fall into the same bucket and form candidate pairs, which we need to test for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import scipy.spatial.distance as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "# define functions to genrate random vectors (which can be treated as the normal vector of a particular hyperplanes)\n",
    "# for each document, get its dot product with each of the random vector, if the result is >= 0, return 1 as signature\n",
    "# else return 0 as signature, finally, the length of a sketch for a document = number of random vectors \n",
    "def signature_bit(featureMatrix, planes):\n",
    "    \"\"\"\n",
    "    LSH signature generation using random projection\n",
    "    :parameter featureMatrix: shape(n_doc, n_features)\n",
    "    :parameter planes: shape(n_features, n_signature)\n",
    "    return the signature bits (sketch) for all vectors, shape(n_doc, n_signature)\"\"\"\n",
    "    sigs = []\n",
    "    for i in featureMatrix:\n",
    "        a = [0]*planes.shape[1]\n",
    "        for j in range(planes.shape[1]):\n",
    "            if np.dot(i, planes[:,j]) >= 0:\n",
    "                a[j] = 1\n",
    "        sigs.append(a)        \n",
    "    sigMatrix = np.vstack(sigs) \n",
    "    return sigMatrix\n",
    "\n",
    "def sketch(featureMatrix, n_feature, n_sig):\n",
    "    \"\"\"\n",
    "    Return sketch vector for all documents\n",
    "    parameter n_feature: \n",
    "    parameter featureMatrix: shape is (nDocs, nFeatures)\n",
    "    parameter n_sig: number of signatures for each document\n",
    "    \"\"\"\n",
    "    ref_planes = np.random.randn(n_sig, n_feature) # randomly generate n_sig vectors, dimesnion=n_feature, values in N(0,1)\n",
    "    sketches = signature_bit(featureMatrix, ref_planes.T).T # transpose the result to get (n_signature,n_doc) matrix\n",
    "    return sketches\n",
    "\n",
    "\n",
    "\n",
    "# find candidate pairs by hashing the siganitures values, \n",
    "# if in at least one of the bands, the two documents hash to the same bucket, they form a candidate pair\n",
    "\n",
    "def LSH(sigMatrix, b, r):\n",
    "    \"\"\"\n",
    "    map similar documents into the same hash bucket\n",
    "    param signMatrix: signature matrix with shape(n_signature, n_doc)\n",
    "    param b: the number of bands\n",
    "    param r: the number of rows in a band\n",
    "    return the hash bucket: a dictionary, key is hash value, value is column number\n",
    "    \"\"\"\n",
    "    hashBuckets = {}\n",
    "    begin, end = 0, r\n",
    "    count = 0\n",
    "    while end <= sigMatrix.shape[0]:\n",
    "        count += 1\n",
    "        for col in range(sigMatrix.shape[1]):\n",
    "            h_fn = hashlib.md5()\n",
    "            band = str(sigMatrix[begin: begin + r, col]) + str(count)\n",
    "            h_fn.update(band.encode())\n",
    "            tag = h_fn.hexdigest() # use hash value as bucket tag\n",
    "            \n",
    "            if tag not in hashBuckets:\n",
    "                hashBuckets[tag] = [col]\n",
    "            elif col not in hashBuckets[tag]:\n",
    "                hashBuckets[tag].append(col)\n",
    "        begin += r\n",
    "        end += r\n",
    "    return hashBuckets\n",
    "\n",
    "def find_pairs(li):\n",
    "    \"\"\"find combination pairs of similar items in the same bucket\"\"\"\n",
    "    pairs = set()\n",
    "    for i in combinations(li,2):\n",
    "        pairs.add(i)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv('data/movie_norm_featureMatrix_sample.csv',index_col=0)\n",
    "combined = pd.read_csv('data/combined_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4204)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureMatrix = np.array(feature_df)\n",
    "featureMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_feature = featureMatrix.shape[1]       # number of features)\n",
    "n_sig = 200                            # number of random vectors of signatures for each doc\n",
    "sketches = sketch(featureMatrix,n_feature,n_sig)\n",
    "LSH_buckets = LSH(sketches,20,10) # number of bands = 20, number of rows=10 in each band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12923"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LSH_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113297"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candi_pairs = set()\n",
    "for key, value in LSH_buckets.items():\n",
    "    if len(value) >= 2:\n",
    "        pairs = find_pairs(value)\n",
    "        candi_pairs.update(pairs)\n",
    "len(candi_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df = pd.DataFrame(sorted(candi_pairs),columns=['movie1','movie2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct a reverse map of indices and movie titles\n",
    "indices = pd.Series(combined.index, index=combined['title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 after having the buckets and candidate pairs of similar movies, we can  \n",
    "### A) recommend most popular movies in the same bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_popula(m_idx):\n",
    "    return feature_df.iloc[m_idx,-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df['popularity'] = pair_df.apply(lambda x: give_popula(x['movie2']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df_new = pair_df.set_index(['movie1','movie2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations_popular(title):\n",
    "    \"\"\"\n",
    "    Given a movie title, we first find it's movieID, then get the movies that are in the same bucket of that movie,\n",
    "    sort these movies based on popularity and return the top 10 with highest popularity.\n",
    "\n",
    "    \"\"\"\n",
    "    idx = indices[title]\n",
    "    try:\n",
    "        sim_scores = pair_df_new.loc[idx,:].sort_values(by=['popularity'],ascending=False)\n",
    "    except:\n",
    "        print('No similar movies based on the given movie')\n",
    "        print('Recommend the most popular movies')\n",
    "        popular = feature_df.sort_values(by=['popularity'],ascending=False)\n",
    "        movie_indices = popular.index[:10]\n",
    "        recom_list = []\n",
    "        for i in movie_indices:\n",
    "            a = combined.loc[i,'title']\n",
    "            recom_list.append(a)\n",
    "        return recom_list\n",
    "    \n",
    "    if len(sim_scores) >=10:\n",
    "        movie_indices = list(sim_scores.index[:10])\n",
    "    else:\n",
    "        movie_indices = list(sim_scores.index)\n",
    "    recom_list = []\n",
    "    for i in movie_indices:\n",
    "        a = combined.loc[i,'title']\n",
    "        recom_list.append(a)\n",
    "    \n",
    "    return recom_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Star Wars',\n",
       " 'The Godfather: Part II',\n",
       " 'Raiders of the Lost Ark',\n",
       " 'The Empire Strikes Back',\n",
       " 'Addams Family Values',\n",
       " 'Robin Hood: Men in Tights',\n",
       " 'The Good, the Bad and the Ugly',\n",
       " 'Field of Dreams',\n",
       " 'The English Patient',\n",
       " 'The Princess Bride']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations_popular('Jumanji')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) recommend most similar movies in the same bucket"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this code took a very long time and large momery, this method is not feasible \n",
    "#pair_df['cosine_sim'] = 1-dist.cosine(feature_df.iloc[pair_df['movie1'],:],feature_df.iloc[pair_df['movie2'],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function calculate cosine similarities \n",
    "def cal_cosine(m1_idx,m2_idx):\n",
    "    return 1-dist.cosine(feature_df.iloc[m1_idx,:],feature_df.iloc[m2_idx,:])\n",
    "\n",
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations_cosine(title):\n",
    "    \"\"\"\n",
    "    Given a movie title, we first find it's index in the movie profile (not movieID), \n",
    "    then get the movies that are in the same bucket of that movie,\n",
    "    calculate cosine similarities and sort these movies based on cosine similarities and \n",
    "    return the top 10 with highest similarities.\n",
    "    If the given movie has no other item in the same bucket, return the most popular movies\n",
    "\n",
    "    \"\"\"\n",
    "    idx = indices[title]\n",
    "    try:\n",
    "        sim_df = pair_df[pair_df['movie1']==idx].copy()\n",
    "        sim_df['cosine'] = sim_df.apply(lambda x: cal_cosine(x['movie1'], x['movie2']),axis=1)\n",
    "        sim_scores = sim_df['cosine'].sort_values(ascending=False)\n",
    "    except:\n",
    "        print('No similar movies based on the given movie')\n",
    "        print('Recommend the most popular movies')\n",
    "        popular = feature_df.sort_values(by=['popularity'],ascending=False)\n",
    "        movie_indices = popular.index[:10]\n",
    "        recom_list = []\n",
    "        for i in movie_indices:\n",
    "            a = combined.loc[i,'title']\n",
    "            recom_list.append(a)\n",
    "        return recom_list\n",
    "    \n",
    "    if len(sim_scores) >=10:\n",
    "        movie_indices = list(sim_scores.index[:10])\n",
    "    else:\n",
    "        movie_indices = list(sim_scores.index)\n",
    "    recom_list = []\n",
    "    for i in movie_indices:\n",
    "        a = combined.loc[i,'title']\n",
    "        recom_list.append(a)\n",
    "    \n",
    "    return recom_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bushwhacked',\n",
       " 'Disclosure',\n",
       " 'Before the Rain',\n",
       " 'Living in Oblivion',\n",
       " 'First Knight',\n",
       " 'The Neon Bible',\n",
       " 'Boys on the Side',\n",
       " 'Moonlight and Valentino',\n",
       " 'The Browning Version',\n",
       " 'The Babysitter']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations_cosine('Jumanji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bdata]",
   "language": "python",
   "name": "conda-env-bdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
